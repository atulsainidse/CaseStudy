---
title: "Internet Advertisements"
output: html_notebook
---

### Data Set Information:
This dataset represents a set of possible advertisements on Internet pages. The features encode the geometry of the image (if available) as well as phrases occuring in the URL, the image’s URL and alt text, the anchor text, and words occuring near the anchor text. The task is to predict whether an image is an advertisement (“ad”) or not (“nonad”).

### Attribute Information:
There are 3 continuous attributes(height,width,aspect ratio) -28% data is missing for each continuous attribute

Others are Binary

### Context
The task is to predict whether an image is an advertisement (“ad”) or not (“nonad”).

### Content
There are 1559 columns in the data.Each row in the data represent one image which is tagged as ad or nonad in the last column.column 0 to 1557 represent the actual numerical attributes of the images
```{r}
library(readr)
library(dplyr)
library(data.table)
library(corrplot)
library(GGally)
library(ggplot2)
library(e1071)
library(reshape)
library(caret)
library(ROSE)
```


#### 1. Read Data
```{r}
data <- read.csv("add.csv", row.names = 1)
cat("Total number of rows and columns", dim(data))
```


Function to trim leading and trailing whitespace

```{r}
# returns string w/o leading or trailing whitespace
trim <- function (x) gsub("^\\s+|\\s+$", "", x)
```

Printing 11th row of data
```{r}
data[11,]
```
There are some missing values in data, which are not marked as NA.

Convert Data type of height, width and aspect ratio columns to Numeric as they are contineous variables

```{r}
data$X0 <- as.numeric(as.character(data$X0))
data$X1 <- as.numeric(as.character(data$X1))
data$X2 <- as.numeric(as.character(data$X2))
data$X3 <- as.numeric(as.character(data$X3))
#data$X0[trim(dataX0) =='?'] <- NA


```

#### Remove NA value rows from data. These values can be imputed as well.

```{r}
data <- na.omit(data)
cat("Total number of rows and columns after removing unknown data", dim(data))
```

```{r}
colnames(data)[1] <- "height"
colnames(data)[2] <- "width"
colnames(data)[3] <- "aspect"
colnames(data)[1559] <- "class"
data$height = as.numeric(data$height)
data$width = as.numeric(data$width)
data$aspect = as.numeric(data$aspect)

```


```{r}
class(data$X6)
data[11,]
```


##### Verify Still any NA value is remaining
```{r}
which(rowSums(is.na(data))==ncol(data))
```

```{r}
summary(data$class)
```

```{r}
num_var = c("height", "width", "aspect")
dt_train_num <- data[,num_var]
```

#### Correlation Plot
```{r}
correlations <- cor(dt_train_num)
corrplot(correlations, method="square", order="hclust")
```
##### Independent variables are not highly correlated.

### Density Plots
#### Plot Height
```{r}

ggplot(data, aes(x=height)) + geom_density(aes(group=class, colour=class, fill=class), alpha=0.3)
```
##### height seems to be left skewed. 

#### apply log transformation
```{r}
data$height <- log(data$height)
ggplot(data, aes(x=height)) + geom_density(aes(group=class, colour=class, fill=class), alpha=0.3)
```

#### Plot Width
```{r}
ggplot(data, aes(x=width)) + geom_density(aes(group=class, colour=class, fill=class), alpha=0.3)
```


##### apply sqrt transformation
```{r}
data$width <- sqrt(data$width)
ggplot(data, aes(x=width)) + geom_density(aes(group=class, colour=class, fill=class), alpha=0.3)
```

#### Plot Aspect Ratio
```{r}
ggplot(data, aes(x=aspect)) + geom_density(aes(group=class, colour=class, fill=class), alpha=0.3)
```
##### aspect ration seems to be right skewed. Also there is outlier

To treat outlier boxplot can also be plot.

#### apply log transformation on height
```{r}
data$aspect <- log10(data$aspect)
ggplot(data, aes(x=aspect)) + geom_density(aes(group=class, colour=class, fill=class), alpha=0.3)
```



#### Stratified Sampling
```{r}
data$class = as.factor(data$class)
is.factor(data$class)
summary(data$class)
```
It seems to be imbalanced class problem, we need to work on stratified sampling.
```{r}
set.seed(42)
index <- createDataPartition(data$class, p = 0.8, list = FALSE)
train_data <- data[index, ]
test_data  <- data[-index, ]
```

```{r}
cat('size of train data', dim(train_data))
cat('\nsize of test data', dim(test_data))
cat('\nSummary of train class', summary(train_data$class))
```

#### This seems to be imbalanced class problem, we need to work on stratified sampling.
```{r}
cat('% of class distribution')
#check classes distribution
prop.table(table(train_data$class))
```
As we see there are only 14% ad and 85% nonad class in train data, This is imbalanced data.


Tried Basic Decision Tree algorithm as data is imbalanced, and not sure if it is linearly seperable
```{r}
library(rpart)

treeimb <- rpart(class ~ ., data = train_data)
pred.class <- predict(treeimb, newdata = test_data)

```

```{r}
accuracy.meas(test_data$class, pred.class[,2])

```

```{r}
roc.curve(test_data$class, pred.class[,2], plotit = F)

```



#####We’ll use the sampling techniques and try to improve this prediction accuracy.

#### Over sampling

This method over instructs the algorithm to perform oversampling. As the original dataset had 1583 good observations, this method is used to oversample minority class until it reaches 1583. The dataset has a total of 454K samples. This can be attained using method = “over”.
```{r}
data_balanced_over <- ovun.sample(class ~ ., data = train_data, method = "over",N = 3166)$data
table(data_balanced_over$class)
```


### Under Sampling

This method functions similar to the oversampling method and is done without replacement. In this method, good transactions are equal to fraud transactions. Hence, no significant information can be obtained from this sample. This can be attained using method = “under”.
```{r}
data_balanced_under <- ovun.sample(class ~ ., data = train_data, method = "under", N = 610, seed = 1)$data
table(data_balanced_under$class)
```

### Both Sampling

This method is a combination of both oversampling and undersampling methods. Using this method, the majority class is undersampled without replacement and the minority class is oversampled with replacement. This can be attained using method = “both”.
```{r}
data_balanced_both <- ovun.sample(class ~ ., data = train_data, method = "both", p=0.5, N=1200, seed = 1)$data
table(data_balanced_both$class)
```


### Synthetic Minority Over-Sampling Technique (SMOTE) Sampling (ROSE Sampling)

ROSE sampling method generates data synthetically and provides a better estimate of original data.

This method is used to avoid overfitting when adding exact replicas of minority instances to the main dataset.

For example, a subset of data from the minority class is taken. New synthetic similar instances are created and added to the original dataset.


```{r}
data.rose <- ROSE(class ~ ., data = train_data, seed = 1)$data
table(data.rose$class)
```
Apply simple Decision Tree Algorithm for different sampling techniques


```{r}

tree.rose <- rpart(class ~ ., data = data.rose)
tree.over <- rpart(class ~ ., data = data_balanced_over)
tree.under <- rpart(class ~ ., data = data_balanced_under)
tree.both <- rpart(class ~ ., data = data_balanced_both)


```
make predictions on unseen data
```{r}

#make predictions on unseen data
pred.tree.rose <- predict(tree.rose, newdata = test_data)
pred.tree.over <- predict(tree.over, newdata = test_data)
pred.tree.under <- predict(tree.under, newdata = test_data)
pred.tree.both <- predict(tree.both, newdata = test_data)

```

##### Accuracy for different type of sampling

```{r}
accuracy.meas(test_data$class, pred.tree.rose[,1])
```

```{r}
accuracy.meas(test_data$class, pred.tree.over[,1])
```

```{r}
accuracy.meas(test_data$class, pred.tree.under[,1])
```

```{r}
accuracy.meas(test_data$class, pred.tree.both[,1])
```

#### Some more operations to perform -

1. Outlier removal from contineous variables.

2. Check if data is linearly seperable, if yes then try loggistic regression.

3. Remove sparcity of binary data using Dimention Reduction Techniques and apply classifier on Reduced dimention.

## To be continued......
### This is not the end, accuracy of model can be improved by trying different types of model and hyperparameter tuning, with different classification algorithms.